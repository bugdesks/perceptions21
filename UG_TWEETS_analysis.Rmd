---
title: "CIPESA Research UG Tweets Perceptions and Sentiments Analysis"
author: "researchers@bugdesks"
date: November 2020
output: 
  html_document:
    toc: true
    toc_depth: 3
    fig_width: 6
    fig_height: 6
cache: yes
---

#=============
# LOAD R LIBRARIES AND PACKAGES. 
#=============
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(janitor) # janitizing the data quick manips. 
library(hutils)
library(rtweet) # read in the twitter data file while keeping the format. 
# library(MASS)  # get a dataset for testing format and manipulation

```
#=============
# GET RAW DATA
#=============

```{r}
cipesa <- read_tsv("~/CIPESA/geotest_edit.csv") # data downloaded from twitter from folder.  

# TODO: seems tab separated from the dump, follow up with @Joachim if this standard. 
str(cipesa) # check all the data types for analysis. 

```

#=============
# CLEANING HASHTAGS DATA 
#=============

```{r}
# script here shorturl.at/myW28
cipesa_HASHTAG <- cipesa 
hashtag_pat <- "#[a-zA-Z0-9_-ー\\.]+"
hashtag <- str_extract_all(cipesa_HASHTAG$tweet, hashtag_pat)

# clean up, unlist to proper text. 
hashtag_word <- unlist(hashtag)
hashtag_word <- tolower(hashtag_word)
hashtag_word <- gsub("[[:punct:]ー]", "", hashtag_word)

# analyse basics data. 
hashtag_count <- table(hashtag_word)
top_20_freqs <- sort(hashtag_count, decreasing = TRUE)[1:20]

# Visualize the top n tweets. 
as.data.frame(hashtag_word) %>%
  count(hashtag_word, sort = TRUE) %>%
  mutate(hashtag_word = reorder(hashtag_word, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = hashtag_word, y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Count",
       y = "Hashtag",
       title = "Top 20 Popular Hashtags from Uganda ")

```
#=============
# CLEANING DATA FRAME AND FORMATS
# EXPLORATORY DATA ANALYSIS ON MAIN DATA POINTS
#=============

```{r}
# script here shorturl.at/pCDH8
cipesa_CLEAN <- cipesa %>%
  dplyr::select(conversation_id, created_at, date, time, username,
                tweet,retweets_count, likes_count, hashtags, geo, photos, video) %>%
  dplyr::filter(hashtags != "[]")

# plot testing by date/time see trends over the day. 
ts_plot(cipesa_CLEAN, "hours") +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets trending Uganda hashtag",
       subtitle = paste0(min(cipesa_CLEAN$created_at), " to ", max(cipesa_CLEAN$created_at)),
       caption = "Data collected from Twitter's UG tweeps") +
  theme_minimal()

# check tweet locations, many turned off though. 
cipesa_LINK <- cipesa %>%
   # filter(!is.na(urls)) %>% 
  filter(urls != "[]") %>%
  count(urls, sort = TRUE) %>% 
  top_n(20) -> cipesa_LINK

# most re-tweeted tweets. 
# This will come in handy to get the top discussions across the weeks/days.
cipesa_RETWEETED <- cipesa %>%
  dplyr::arrange(desc(retweets_count)) %>%
  #dplyr::slice(120) %>% 
  dplyr::select(created_at, username, tweet, retweets_count) %>%
  head()
cipesa_RETWEETED

# most liked tweet 
cipesa_LIKED <- cipesa %>%
  dplyr::arrange(desc(likes_count)) %>%
  #dplyr::slice(120) %>% 
  dplyr::select(created_at, username, tweet, likes_count) %>%
  head()
cipesa_LIKED

# top tweeters or tweeps
cipesa_TWEEP <- cipesa %>%
  dplyr::count(username, sort = TRUE) %>%
  top_n(10) %>%
  mutate(screen_name = paste0("@", username))
cipesa_TWEEP

# top emoji or emoticon used. 
library(emo)
cipesa_EMO <- cipesa %>%
  mutate(emoji = ji_extract_all(tweet)) %>%
  unnest(cols = c(emoji)) %>%
  count(emoji, sort = TRUE) %>%
  top_n(10)
cipesa_EMO

# top hashtags across. 
cipesa_HASHTAGS <- cipesa %>%
  tidytext::unnest_tokens(hashed, tweet, "tweets", to_lower = FALSE) %>% # splits across the rows. 
  filter(str_detect(hashed, "^#")) %>%
  count(hashed, sort = TRUE) %>%
  top_n(20)
cipesa_HASHTAGS

# top mentions. 
# Let's also tokenise the text each tweet and use str_detect()
cipesa_MENTIONS <- cipesa %>%
  tidytext::unnest_tokens(mentions, tweet, "tweets", to_lower = FALSE) %>%
  filter(str_detect(mentions, "^@")) %>%  
  count(mentions, sort = TRUE) %>%
  top_n(20)
cipesa_MENTIONS

# top words being used across. 
# TODO: check issue with the tidytext::stop_words why won't work without explicit mention. 
library(wordcloud)

cipesa_MENTIONS <- cipesa %>%
  mutate(tweet = str_remove_all(tweet, "&amp;|&lt;|&gt;"),
         tweet = str_remove_all(tweet, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
         tweet = str_remove_all(tweet, "[^\x01-\x7F]")) %>% 
  tidytext::unnest_tokens(word, tweet, token = "tweets", to_lower = FALSE) %>%
  filter(!word %in% tidytext::stop_words$word,
        !word %in% str_remove_all(tidytext::stop_words$word, "'"),
        str_detect(word, "[a-z]"),
        !str_detect(word, "^#"),         
        !str_detect(word, "@\\S+")) %>%
  count(word, sort = TRUE)
cipesa_MENTIONS
# visualize the cloud
cipesa_MENTIONS %>% 
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = "#F29545"))

```


#=============
# MERGING DETAILS
# HASHTAGS DATA 
#=============

```{r}
# TODO: Fix the data loss truncation with BigInts??????
# script here 
cipesa_EXPLORE_HASHTAGS <- cipesa %>%
  dplyr::select(-c(photos, quote_url), -c(source:trans_dest)) %>% #TODO: better script clean up unnecessary columns.
  dplyr::mutate(id = as.character(id), conv_id = as.character(conversation_id), user_id = as.character(user_id)) %>%
  tidytext::unnest_tokens(hashtag, tweet, "tweets", to_lower = FALSE) %>% # splits across the rows. 
  filter(str_detect(hashtag, "^#"))


# Here add match hashtag table for classification of affiliation. 
# Options: ruling, opposition, general. 
# TODO: @bugdesks load the data from csv or google spreadsheet. 
cipesa_HASHTAGS_AFFILIATION <- cipesa_EXPLORE_HASHTAGS %>%
  dplyr::count(hashtag, sort = TRUE) %>%
  dplyr::select(hashtag)

# write the flagged hashtags for research assistant population
# TODO: @bugdesks ONLY FOR SHARING WITH only research assistants. 
write.csv(V1_UG_ELECTIONS_HASHTAGS, file="~/CIPESA/cipesa_HASHTAGS_AFFILIATION.csv",
          row.names=FALSE)

# read the modified file from research assistants with the new affiliation scoring. 
#TODO: @bugdesks make this read from google spreadsheet and not local file as work with research assistants. 
V1_UG_ELECTIONS_HASHTAGS <- read_csv("~/CIPESA/V1_UG_ELECTIONS_HASHTAGS.csv") # data downloaded from twitter from folder.  

# Merge into the main df: cipesa_HASHTAGS_AFFILIATION to populate the data with respective affiliations across. 
# Annotate and clean variables required for analysis. 

cipesa_HASHTAGS_AFFILIATION_JOINED <- left_join(cipesa_EXPLORE_HASHTAGS, V1_UG_ELECTIONS_HASHTAGS, by = 'hashtag')
  

```